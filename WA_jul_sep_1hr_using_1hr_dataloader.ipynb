{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b34f1a-81cf-45a8-93d7-645bdfbaf468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 16:57:19.720683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740502639.731464   21354 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740502639.734551   21354 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 16:57:19.746208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/keras/engine/training_arrays_v1.py:37: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  from scipy.sparse import issparse  # pylint: disable=g-import-not-at-top\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers\n",
    "import tensorflow.keras.metrics\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import pickle \n",
    "from datetime import date\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "from ndays import numOfDays\n",
    "import glob\n",
    "import calendar\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e27421-9474-4c0e-95d7-d789f93b8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain and time period\n",
    "start_lat = 5  #\n",
    "end_lat = 25   #\n",
    "start_lon = -18 #\n",
    "end_lon = -2  #\n",
    "start_year = '2006'\n",
    "end_year = '2019'\n",
    "start_month = '08' #6\n",
    "end_month = '10'  #9\n",
    "start_day = '01'\n",
    "end_day = '31'\n",
    "lead_time = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99bdb64-f726-4317-be4c-ff6dd6e7d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get native MSG grid (core)\n",
    "coords_filename= '../nxny1640_580_nxnyds164580_blobdx0.04491576_area4_n23_20_32.nc'#[0]  # this is /prj/Africa_cloud/geoloc/*.npz on the Linux system\n",
    "msg_file = xr.open_dataset(coords_filename).squeeze() # pick any convective core file from ch9_wavelet. \n",
    "mlon = msg_file['lons_mid'].values\n",
    "mlat = msg_file['lats_mid'].values\n",
    "# find core indices using one file\n",
    "lat_ind = np.where((mlat[:,1]>=start_lat) & (mlat[:,1]<=end_lat))[0]\n",
    "lon_ind = np.where((mlon[1,:]>=start_lon) & (mlon[1,:]<=end_lon))[0]\n",
    "lat = mlat[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "lon = mlon[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421aa628-02ef-4960-be00-f4d960d034ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5844\n",
      "5844\n"
     ]
    }
   ],
   "source": [
    "# read in cores data\n",
    "list_of_input_file_names = []\n",
    "\n",
    "for m in range(int(start_month),int(end_month)+1,1): # 1 to include the end month\n",
    "    if m<10:\n",
    "        month = str(m).zfill(2)\n",
    "    else:\n",
    "        month = str(m)  \n",
    "    for d in range(1,calendar.monthrange(int(start_year), m)[1]+1,1): # 1 to include the end month\n",
    "        if d<10:\n",
    "            day = str(d).zfill(2)\n",
    "        else:\n",
    "            day = str(d)  \n",
    "            #core_filename= '/prj/nflics/hist_cores/2008/06/01/Hist_cores_wa_200806011045.nc'\n",
    "        dir_name = '/prj/nflics/hist_cores/'+start_year+'/'+month+'/'+day+'/'\n",
    "        all_file_names=sorted(glob.glob(dir_name+\"Hist_cores_wa_*.nc\"));\n",
    "        list_of_input_file_names.append(all_file_names) # all days in month\n",
    "                  \n",
    "list_of_input_files=[]                \n",
    "for a in range(0,len(list_of_input_file_names),1):\n",
    "    list_of_input_files = list_of_input_files+list_of_input_file_names[a]\n",
    "\n",
    "list_of_output_files=list_of_input_files[3*4*lead_time:]\n",
    "list_of_input_files=list_of_input_files[0:-3*4*lead_time]\n",
    "# for i=0 the predition time will be to-2 (i=0), to-1(i=4), to(i=8) to predict at to+xhr (here x=1 so i=12)\n",
    "\n",
    "print(len(list_of_input_files))\n",
    "print(len(list_of_output_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccca601e-07a2-499f-a596-a089fd821d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a= 8 # 85\n",
    "b= -63 # 98\n",
    "\n",
    "image_height= 512 #lat\n",
    "image_width= 512 #lon\n",
    "num_channels= 3 #  core at t0-, core at t0-1,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf33b8b-4e75-48bc-8fd1-18fd85d6a8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence(i):\n",
    "   # print(i) \n",
    "    # Load 3x4 consecutive input frames\n",
    "    tir = np.zeros((len(lat[:,1]),len(lon[1,:]),3),dtype=float)\n",
    "\n",
    "    for offset in range(3):\n",
    "        with nc.Dataset(list_of_input_files[i + 4*offset]) as ds:\n",
    "            tir_temp =  ds.variables['msg_Tir'][:]\n",
    "            tir[:,:,offset] = tir_temp[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]/10000    \n",
    "\n",
    "    tir_t_0 = tir[a:,:b,:]\n",
    "    ind_tir = np.where(tir_t_0>-0.01)\n",
    "    tir_t_0[ind_tir] = 0\n",
    "    tir_t_0[np.isnan(tir_t_0)] = 0\n",
    "    tir_t_0 = np.round(tir_t_0/-173,4)\n",
    "    x_train = tir_t_0\n",
    "    \n",
    "    # Load the output frame\n",
    "    #target data\n",
    "    cores = np.zeros((len(lat[:,1]),len(lon[1,:])),dtype=float)\n",
    "    time_core = np.zeros((1)) \n",
    "\n",
    "    #for offset in range(4):\n",
    "    core_filename = list_of_output_files[i]  \n",
    "    ds = xr.open_dataset(core_filename).squeeze() \n",
    "    core_temp = ds['msg_cores'].values\n",
    "    cores = core_temp[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "    time_core = str(core_filename[-15:-3])\n",
    "\n",
    "    ind = np.where(cores>0)\n",
    "    cores[ind] = 1\n",
    "    y_train= cores[a:,:b]\n",
    "    y_train=np.expand_dims(y_train, axis=2)\n",
    "   \n",
    "    prediction_time = time_core\n",
    "    time_of_day_tr= np.zeros((image_height, image_width,1))\n",
    "    time_of_day = float(str(prediction_time)[-6:])/2345\n",
    "    time_of_day_tr[:,:,:]=round(np.sin(time_of_day*math.pi),2)\n",
    "\n",
    "    # define input and output tensors\n",
    "    input_tensor = [x_train,time_of_day_tr]\n",
    "    output_tensor= y_train\n",
    "\n",
    "    return input_tensor, output_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9bdccbd-c425-4852-a351-d6243ca02322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequence_wrapper(i):\n",
    "    return tf.numpy_function(load_sequence, [i], [tf.float32, tf.float32])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba3e6eb-6e69-45fa-8779-aed590eeec10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740502641.780357   21354 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43573 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, concatenate, Conv2D, MaxPooling2D, Activation, UpSampling2D)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "# define conv layer\n",
    "def conv_layer(x_in, filters, gn_num, strides = 1, shape = (image_height,image_width), name=None):\n",
    "    x = Conv2D(filters, 5, strides=strides, padding='same')(x_in)\n",
    "    x = Activation('relu', name = name)(x)\n",
    "   ## x = layers.BatchNormalization()(x)\n",
    "    #x = tfa.layers.GroupNormalization(groups=gn_num, axis=3)(x) # could replace this with BatchNorm\n",
    "    return x\n",
    "\n",
    "\n",
    "# def basic unet structure\n",
    "def unet_basic(input_shape=(image_height, image_width, num_channels), meta_input_shape=(image_height, image_width,1) ,chan_num=num_channels):\n",
    "\n",
    "    inputs = Input(shape=input_shape)    # 256 x 256 x 2\n",
    "    meta_inputs = Input(shape=meta_input_shape)\n",
    "\n",
    "    # downsample\n",
    "    down2 = conv_layer(inputs, 4, 1)     # 256 x 256 x 4\n",
    "    down2_pool = MaxPooling2D((2, 2), strides=None)(down2)   # 128 x 128 x 4\n",
    "\n",
    "    down3 = conv_layer(down2_pool, 8, None) # 128 x 128 x 8\n",
    "    down3_pool = MaxPooling2D((4, 4), strides=None)(down3) # 32 x 32 x 8\n",
    " \n",
    "    center = conv_layer(down3_pool, 16, None) # 32 x 32 x 16\n",
    "    center = conv_layer(center, 8, None) # 32 x 32 x 8\n",
    "  \n",
    "    up3 = UpSampling2D((4, 4))(center) # 128 x 128 x 8\n",
    "    up3 = concatenate([down3, up3], axis=3) # 128 x 128 x 16\n",
    "    up3 = conv_layer(up3, 4, None) # 128 x 128 x 4\n",
    "\n",
    "    up2 = UpSampling2D((2, 2))(up3) # 256 x 256 x 4\n",
    "    up2 = concatenate([down2, up2, meta_inputs], axis=3) # 256 x 256 x 9\n",
    "    up2 = conv_layer(up2, 4, None) # 256 x 256 x 4\n",
    "  \n",
    "    # predict\n",
    "    output = Conv2D(1, (1, 1))(up2)\n",
    "    output1 = layers.Activation('sigmoid', dtype='float32', name='prob_pred')(output)\n",
    "    \n",
    "    # create model object\n",
    "    unet = Model(inputs=[inputs,meta_inputs], outputs=output1, name = 'prob_map_unet')\n",
    "    return unet\n",
    "\n",
    "\n",
    "unet_model = unet_basic(input_shape=(image_height, image_width, num_channels),meta_input_shape=(image_height, image_width,1), chan_num=num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79fdb9df-613b-4ac5-b150-110893d672c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_mean_filter(half_num_rows, half_num_columns, num_channels):\n",
    "    \"\"\"Creates convolutional filter that computes mean.\n",
    "\n",
    "    M = number of rows in filter\n",
    "    N = number of columns in filter\n",
    "    C = number of channels\n",
    "\n",
    "    :param half_num_rows: Number of rows on either side of center.  This is\n",
    "        (M - 1) / 2.\n",
    "    :param half_num_columns: Number of columns on either side of center.  This\n",
    "        is (N - 1) / 2.\n",
    "    :param num_channels: Number of channels.\n",
    "    :return: weight_matrix: M-by-N-by-C-by-C numpy array of filter weights.\n",
    "    \"\"\"\n",
    "\n",
    "    num_rows = 2 * half_num_rows + 1\n",
    "    num_columns = 2 * half_num_columns + 1\n",
    "    weight = 1. / (num_rows * num_columns)\n",
    "\n",
    "    return np.full(\n",
    "        (num_rows, num_columns, num_channels, num_channels), weight,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "def FSS_loss(target_tensor, prediction_tensor):\n",
    "    \n",
    "    half_window_size_px=2\n",
    "    use_as_loss_function=True \n",
    "    #mask_matrix\n",
    "    function_name=None\n",
    "    test_mode=False\n",
    "    \"\"\"Fractions skill score (FSS).\n",
    "\n",
    "    M = number of rows in grid\n",
    "    N = number of columns in grid\n",
    "\n",
    "    :param half_window_size_px: Number of pixels (grid cells) in half of\n",
    "        smoothing window (on either side of center).  If this argument is K, the\n",
    "        window size will be (1 + 2 * K) by (1 + 2 * K).\n",
    "    :param use_as_loss_function: Boolean flag.  FSS is positively oriented\n",
    "        (higher is better), but if using it as loss function, we want it to be\n",
    "        negatively oriented.  Thus, if `use_as_loss_function == True`, will\n",
    "        return 1 - FSS.  If `use_as_loss_function == False`, will return just\n",
    "        FSS.\n",
    "    :param mask_matrix: M-by-N numpy array of Boolean flags.  Grid cells marked\n",
    "        \"False\" are masked out and not used to compute the loss.\n",
    "    :param function_name: Function name (string).\n",
    "    :param test_mode: Leave this alone.\n",
    "    :return: loss: Loss function (defined below).\n",
    "    \"\"\"\n",
    "\n",
    "    weight_matrix = _create_mean_filter(\n",
    "        half_num_rows=half_window_size_px,\n",
    "        half_num_columns=half_window_size_px, num_channels=1\n",
    "    )\n",
    "        \n",
    "    \"\"\"Computes loss (fractions skill score).\n",
    "\n",
    "        :param target_tensor: Tensor of target (actual) values.\n",
    "        :param prediction_tensor: Tensor of predicted values.\n",
    "        :return: loss: Fractions skill score.\n",
    "    \"\"\"\n",
    "\n",
    "    smoothed_target_tensor = K.conv2d(\n",
    "        x=target_tensor, kernel=weight_matrix,\n",
    "        padding='same', strides=(1, 1), data_format='channels_last'\n",
    "    )\n",
    "\n",
    "    smoothed_prediction_tensor = K.conv2d(\n",
    "        x=prediction_tensor, kernel=weight_matrix,\n",
    "        padding='same', strides=(1, 1), data_format='channels_last'\n",
    "    )\n",
    "\n",
    "    #smoothed_target_tensor = smoothed_target_tensor * eroded_mask_matrix\n",
    "    #smoothed_prediction_tensor = smoothed_prediction_tensor * eroded_mask_matrix)\n",
    "\n",
    "    actual_mse = K.mean(\n",
    "        (smoothed_target_tensor - smoothed_prediction_tensor) ** 2\n",
    "    )\n",
    "    reference_mse = K.mean(\n",
    "        smoothed_target_tensor ** 2 + smoothed_prediction_tensor ** 2\n",
    "    )\n",
    "\n",
    "    if use_as_loss_function:\n",
    "        return actual_mse / reference_mse\n",
    "\n",
    "    return 1. - actual_mse / reference_mse\n",
    "\n",
    "    if function_name is not None:\n",
    "        loss.__name__ = function_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a199c2e-e5ad-4490-b27c-3a9d3b3e35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.engine import data_adapter\n",
    "\n",
    "def _is_distributed_dataset(ds):\n",
    "    return isinstance(ds, data_adapter.input_lib.DistributedDatasetSpec)\n",
    "\n",
    "data_adapter._is_distributed_dataset = _is_distributed_dataset\n",
    "\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f589131c-a273-4eff-8688-1e28d86698c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 2. Build dataset\n",
    "dataset = tf.data.Dataset.range(len(list_of_input_files) - 3*4) \n",
    "#dataset = dataset.shuffle(buffer_size=100)  # shuffle if desired\n",
    "#dataset = dataset.map(load_sequence, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.map(load_sequence_wrapper, num_parallel_calls=1) #tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(8)                 # batch size\n",
    "#dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a78d8f0-cca1-49e5-b530-884e67c00074",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=tensorflow.keras.optimizers.Adam(),\n",
    "                  loss=FSS_loss,\n",
    "                  metrics=[tf.keras.metrics.Accuracy()])\n",
    "#                  metrics=metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f2da9c-6d8d-443e-8ef2-20d944f45705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py:82: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(values, dtype=dtype, order=order)\n",
      "2025-02-25 16:57:22.524459: W tensorflow/core/framework/op_kernel.cc:1829] INVALID_ARGUMENT: ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n",
      "    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n",
      "    return np.asarray(values, dtype=dtype, order=order)\n",
      "\n",
      "ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "\n",
      "\n",
      "2025-02-25 16:57:22.524700: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n",
      "    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n",
      "    return np.asarray(values, dtype=dtype, order=order)\n",
      "\n",
      "ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 1)\n",
      "(512, 512, 1)\n",
      "(512, 512, 3)\n",
      "(512, 512, 1)\n",
      "(512, 512, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 16:57:22.662271: W tensorflow/core/framework/op_kernel.cc:1829] INVALID_ARGUMENT: ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n",
      "    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n",
      "    return np.asarray(values, dtype=dtype, order=order)\n",
      "\n",
      "ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\nTraceback (most recent call last):\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n    return [self._convert(x) for x in ret]\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n    return [self._convert(x) for x in ret]\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n    return np.asarray(values, dtype=dtype, order=order)\n\nValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m epochs \u001b[38;5;241m=\u001b[39m  \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Fit the model to the training data.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model_history \u001b[38;5;241m=\u001b[39m \u001b[43munet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#    callbacks=[early_stopping, reduce_lr],\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:129\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[0;32m--> 129\u001b[0m             one_step_on_data(\u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     empty_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\nTraceback (most recent call last):\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n    return [self._convert(x) for x in ret]\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n    return [self._convert(x) for x in ret]\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n\n  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n    return np.asarray(values, dtype=dtype, order=order)\n\nValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 1)\n",
      "(512, 512, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 16:57:22.896801: W tensorflow/core/framework/op_kernel.cc:1829] INVALID_ARGUMENT: ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in __call__\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 277, in <listcomp>\n",
      "    return [self._convert(x) for x in ret]\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 228, in _convert\n",
      "    result = numpy_compat.np_asarray(value, dtype=dtype, order=\"C\")\n",
      "\n",
      "  File \"/home/jawahm/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/numpy_compat.py\", line 82, in np_asarray\n",
      "    return np.asarray(values, dtype=dtype, order=order)\n",
      "\n",
      "ValueError: could not broadcast input array from shape (512,512,3) into shape (512,512)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs =  20\n",
    "\n",
    "# Fit the model to the training data.\n",
    "model_history = unet_model.fit(dataset,epochs=epochs)\n",
    "#    callbacks=[early_stopping, reduce_lr],\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
