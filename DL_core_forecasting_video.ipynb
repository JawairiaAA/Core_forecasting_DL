{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f20271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 16:28:22.756483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "#from tensorflow.keras.models import Sequential\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcbaf5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from u_interpolate_small import regrid_irregular_quick\n",
    "from datetime import date\n",
    "import u_interpolate_small as uint\n",
    "from ndays import numOfDays\n",
    "import glob\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import pickle \n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa0bf788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define domain and time period\n",
    "start_lat = 5 \n",
    "end_lat = 10\n",
    "start_lon = -10\n",
    "end_lon = 0\n",
    "start_year = '2010'\n",
    "start_month = '06'\n",
    "end_year = '2010'\n",
    "end_month = '09'\n",
    "start_day = '01'\n",
    "end_day = '30'\n",
    "\n",
    "# find ndays\n",
    "date1 = date(int(start_year), int(start_month), int(start_day))\n",
    "date2 = date(int(end_year), int(end_month), int(end_day))\n",
    "t = numOfDays(date1, date2)+1\n",
    "\n",
    "# define a regular lat/lon grid close to MSG native resolution (0.04 deg)\n",
    "#reg_lat = np.arange(start_lat, end_lat, 0.04)\n",
    "#reg_lon = np.arange(start_lon, end_lon, 0.04) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f01a65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get native MSG grid (core)\n",
    "coords_filename= './nxny1640_580_nxnyds164580_blobdx0.04491576_area4_n23_20_32.nc'#[0]  # this is /prj/Africa_cloud/geoloc/*.npz on the Linux system\n",
    "msg_file = xr.open_dataset(coords_filename).squeeze() # pick any convective core file from ch9_wavelet. \n",
    "mlon = msg_file['lons_mid'].values\n",
    "mlat = msg_file['lats_mid'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb917448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find core indices using one file\n",
    "lat_ind = np.where((mlat[:,1]>=start_lat) & (mlat[:,1]<=end_lat))[0]\n",
    "lon_ind = np.where((mlon[1,:]>=start_lon) & (mlon[1,:]<=end_lon))[0]\n",
    "lat = mlat[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "lon = mlon[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "#lon = lon_fl[(lon_fl>= start_lon) & (lon_fl<=end_lon)]\n",
    "#X, Y = np.meshgrid(lon,lat)\n",
    "\n",
    "cores = np.zeros((t*96,len(lat[:,1]),len(lon[1,:])),dtype=float) #using every fourth value for hourly comparison- 24hr\n",
    "tir = np.zeros((t*96,len(lat[:,1]),len(lon[1,:])),dtype=float) #using every fourth value for hourly comparison- 24hr\n",
    "time_core = np.zeros((t*96)) #using every fourth value for hourly comparison- 24hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b62bc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in input data- TIR and historical core data at t<to\n",
    "#core_filename= '/prj/nflics/hist_cores/2008/06/01/Hist_cores_wa_200806011045.nc'\n",
    "#ds = xr.open_dataset(core_filename).squeeze() # pick any convective core file from ch9_wavelet. \n",
    "#cores = ds['msg_cores'].values# uint.interpolate_data(ds['cores'].values, inds, weights, shape) # interpolation using saved weights for MSG cores       \n",
    "#tir = ds['msg_Tir'].values/10000 # uint.interpolate_data(ds['tir'].values, inds, weights, shape)  # interpolation using saved weights for MSG TIR          \n",
    "#time_core = ds['time']# (core_filename[-15:-3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c717a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11712\n"
     ]
    }
   ],
   "source": [
    "# read in cores data\n",
    "list_of_file_names = []\n",
    "\n",
    "if int(start_year)==int(end_year): \n",
    "    for m in range(int(start_month),int(end_month)+1,1): # 1 to include the end month\n",
    "        if m<10:\n",
    "            month = str(m).zfill(2)\n",
    "        else:\n",
    "            month = str(m)  \n",
    "        for d in range(1,calendar.monthrange(int(start_year), m)[1]+1,1): # 1 to include the end month\n",
    "            if d<10:\n",
    "                day = str(d).zfill(2)\n",
    "            else:\n",
    "                day = str(d)  \n",
    "            #core_filename= '/prj/nflics/hist_cores/2008/06/01/Hist_cores_wa_200806011045.nc'\n",
    "            dir_name = '/prj/nflics/hist_cores/'+start_year+'/'+month+'/'+day+'/'\n",
    "            all_file_names=sorted(glob.glob(dir_name+\"Hist_cores_wa_*.nc\"));\n",
    "            list_of_file_names.append(all_file_names) # all days in month\n",
    "        #if os.path.isfile(all_file_names):      \n",
    "         #   list_of_file_names.append(all_file_names) # all days in month\n",
    "        #else:\n",
    "         #   list_of_file_names.append('file_not_available') # all days in month\n",
    "            #print(all_file_names)\n",
    "             \n",
    "elif int(start_year)<int(end_year):\n",
    "    y=1\n",
    "    for i in range(int(start_year),int(end_year)+1,1):\n",
    "        if y==1:\n",
    "            for m in range(int(start_month),10,1):\n",
    "                if m<10:\n",
    "                    month = str(m).zfill(2)\n",
    "                else:\n",
    "                    month = str(m)  \n",
    "                for d in range(1,calendar.monthrange(i, m)[1]+1,1):\n",
    "                #for d in range(1,30,1): # 1 to include the end month\n",
    "                    if d<10:\n",
    "                        day = str(d).zfill(2)\n",
    "                    else:\n",
    "                        day = str(d)  \n",
    "            #core_filename= '/prj/nflics/hist_cores/2008/06/01/Hist_cores_wa_200806011045.nc'\n",
    "                    dir_name = '/prj/nflics/hist_cores/'+start_year+'/'+month+'/'+day+'/'\n",
    "                    all_file_names=sorted(glob.glob(dir_name+\"Hist_cores_wa_*.nc\"));\n",
    "                    list_of_file_names.append(all_file_names) # all days in month\n",
    "            #print((all_file_names))\n",
    "                #print(len(all_file_names))\n",
    "                y=y+1\n",
    "                month=[]\n",
    "            \n",
    "        else:\n",
    "            for m in range(1,int(end_month)+1,1):\n",
    "                if m<10:\n",
    "                    month = str(m).zfill(2)\n",
    "                else:\n",
    "                    month = str(m)    \n",
    "                for d in range(1,calendar.monthrange(i, m)[1]+1,1):\n",
    "                    if d<10:\n",
    "                        day = str(d).zfill(2)\n",
    "                    else:\n",
    "                        day = str(d)  \n",
    "                    dir_name = '/prj/nflics/hist_cores/'+start_year+'/'+month+'/'+day+'/'\n",
    "                    all_file_names=sorted(glob.glob(dir_name+\"Hist_cores_wa_*.nc\"));\n",
    "                    list_of_file_names.append(all_file_names) # all days in month\n",
    "                  \n",
    "list_of_files=[]                \n",
    "for a in range(0,len(list_of_file_names),1):\n",
    "    list_of_files = list_of_files+list_of_file_names[a]\n",
    "#if int(start_year)<int(end_year):\n",
    " #   list_of_files = list_of_files[0]+list_of_files[1]\n",
    "#else:\n",
    "   # list_of_files = list_of_files[0]#+list_of_files[1]    \n",
    "print(len(list_of_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98976707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010000.nc\n",
      "0\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010015.nc\n",
      "1\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010030.nc\n",
      "2\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010045.nc\n",
      "3\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010100.nc\n",
      "4\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010115.nc\n",
      "5\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010130.nc\n",
      "6\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010145.nc\n",
      "7\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010200.nc\n",
      "8\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010215.nc\n",
      "9\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010230.nc\n",
      "10\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010245.nc\n",
      "11\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010300.nc\n",
      "12\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010315.nc\n",
      "13\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010330.nc\n",
      "14\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010345.nc\n",
      "15\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010400.nc\n",
      "16\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010415.nc\n",
      "17\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010430.nc\n",
      "18\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010445.nc\n",
      "19\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010500.nc\n",
      "20\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010515.nc\n",
      "21\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010530.nc\n",
      "22\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010545.nc\n",
      "23\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010600.nc\n",
      "24\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010615.nc\n",
      "25\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010630.nc\n",
      "26\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010645.nc\n",
      "27\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010700.nc\n",
      "28\n",
      "/prj/nflics/hist_cores/2010/06/01/Hist_cores_wa_201006010715.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(core_filename)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#if os.path.isfile(all_file_names):      \u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore_filename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze() \n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(l)\n\u001b[1;32m      7\u001b[0m core_temp \u001b[38;5;241m=\u001b[39m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmsg_cores\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/xarray/backends/api.py:554\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 554\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/xarray/backends/plugins.py:154\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine, backend \u001b[38;5;129;01min\u001b[39;00m engines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_can_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_spec\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m engine\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:571\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.guess_can_open\u001b[0;34m(self, filename_or_obj)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_remote_uri(filename_or_obj):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mtry_read_magic_number_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# netcdf 3 or HDF5\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m magic_number\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\211\u001b[39;00m\u001b[38;5;124mHDF\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\032\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/xarray/core/utils.py:673\u001b[0m, in \u001b[0;36mtry_read_magic_number_from_path\u001b[0;34m(pathlike, count)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 673\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_magic_number_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/xarray/core/utils.py:661\u001b[0m, in \u001b[0;36mread_magic_number_from_file\u001b[0;34m(filename_or_obj, count)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename_or_obj\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    660\u001b[0m         filename_or_obj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m     filename_or_obj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for l in range(0,len(list_of_files),1):#, min(t*96,len(list_of_files)), 4):#range(len(list_of_files)): # 96 files per day\n",
    "    core_filename = list_of_files[l]#'/prj/Africa_cloud/ch9_wavelet/'+str(i)+'/'+month+'/'  list_of_files[l]\n",
    "    print(core_filename)\n",
    "    #if os.path.isfile(all_file_names):      \n",
    "    ds = xr.open_dataset(core_filename).squeeze() \n",
    "    print(l)\n",
    "    core_temp = ds['msg_cores'].values\n",
    "    cores[l,:,:] = core_temp[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]\n",
    "    #uint.interpolate_data(ds['cores'].values, inds, weights, shape) # interpolation using saved weights for MSG cores       \n",
    "    tir_temp =  ds['msg_Tir'].values/10000\n",
    "    tir[l,:,:] = tir_temp[lat_ind[0]:lat_ind[-1]+1,lon_ind[0]:lon_ind[-1]+1]    \n",
    "    time_core[l] = str(core_filename[-15:-3])\n",
    "    #else:\n",
    "     #   cores[:,:,l] = cores_temp*np.nan  \n",
    "      #  tir[:,:,l] = tir_temp*np.nan   \n",
    "       # time_core[l] = (core_filename[-15:-3])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef605f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## a clean way of plotting - use matplotlib functions directly:\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy\n",
    "def draw_map(ax, data, lon, lat, title=None,  mask_sig=None, quiver=None, contour=None, cbar_label=None, **kwargs):\n",
    "\n",
    "    mapp = ax.contourf(lon, lat, data, transform=ccrs.PlateCarree(), **kwargs)  # this is the actual plot\n",
    "    \n",
    "    ## mask for significance indicator\n",
    "    if mask_sig is not None:\n",
    "         plt.contourf(lon, lat, mask_sig, colors='none', hatches='.',\n",
    "                     levels=[0.5, 1], linewidth=0.1)\n",
    "    \n",
    "    ## quiver list\n",
    "    if quiver is not None:\n",
    "        qu = ax.quiver(quiver['x'], quiver['y'], quiver['u'], quiver['v'], scale=quiver['scale'])\n",
    "    ## additional contour on plot   \n",
    "    if contour is not None:\n",
    "        ax.contour(contour['x'], contour['y'], contour['data'], levels=contour['levels'], cmap=contour['cmap'] )\n",
    "        \n",
    "    \n",
    "    ax.coastlines()   ## adds coastlines\n",
    "    # Gridlines\n",
    "    xl = ax.gridlines(draw_labels=True);   # adds latlon grid lines\n",
    "    xl.xlabels_top = False   ## labels off\n",
    "    xl.ylabels_right = False\n",
    "    plt.title(title)\n",
    "    # Countries\n",
    "    ax.add_feature(cartopy.feature.BORDERS, linestyle='--'); # adds country borders\n",
    "    cbar = plt.colorbar(mapp)  # adds colorbar\n",
    "    cbar.set_label(cbar_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3e5ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = 96*19\n",
    "f=plt.figure(figsize=(15,7))  # this opens a plot window\n",
    "ax = f.add_subplot(111, projection=ccrs.PlateCarree())  # this opens a new plot axis\n",
    "draw_map(ax, tir[day,:,:], lon, lat, levels=np.arange(-100,50,5), cbar_label='TIR [Tb oC]', cmap='jet')\n",
    "ax.contour(lon, lat, cores[day,:,:], levels=[-5,1,50], colors='r')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11fa2aa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# input data\n",
    "# starting at \n",
    "t0=3*4\n",
    "a=51\n",
    "b=111\n",
    "cores_t_minus_xhr = cores[0:-2*t0, a:,b:]\n",
    "cores_t_0 = cores[t0:-t0, a:,b:]\n",
    "tir_t_minus_xhr = tir[0:-2*t0,a:,b:]\n",
    "tir_t_0 = tir[t0:-t0, a:,b:]\n",
    "\n",
    "\n",
    "#target data\n",
    "cores_t_plus_xhr = cores[t0*2:, a:,b:]\n",
    "ind = np.where(cores_t_plus_xhr>0)\n",
    "cores_t_plus_xhr[ind] = 1\n",
    "ind_not = np.where(cores_t_plus_xhr<=0)\n",
    "cores_t_plus_xhr[ind_not] = 0\n",
    "#print(np.unique(cores_t_plus_xhr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb6baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cores_t_0.shape)\n",
    "print(cores_t_plus_xhr.shape)\n",
    "print(len(lat[51:,1]))\n",
    "print(len(lat[1,111:]))\n",
    "\n",
    "#179-128\n",
    "#367-256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variables \n",
    "#with open('WA_subdomain_cores.pkl', 'wb') as file: \n",
    "    #      pickle.dump([cores_t_0,tir_t_0,cores_t_plus_xhr], file) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f917853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a file, where you stored the pickled data\n",
    "file = open('WA_subdomain_cores.pkl', 'rb')\n",
    "data = pickle.load(file)\n",
    "cores_t_0= data[0]\n",
    "tir_t_0= data[1]\n",
    "cores_t_plus_xhr= data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7829fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "num_frames= 24  # 6 hours\n",
    "image_height= len(cores_t_plus_xhr[1,:,1]) #lat\n",
    "image_width= len(cores_t_plus_xhr[1,1,:]) #lon\n",
    "num_channels= 1 #  core at t0-       tir at t0-x, t0, , cores\n",
    "#input_shape = (image_height, image_width, num_channels)\n",
    "#input_shape = (num_frames, image_height, image_width, num_channels)\n",
    "# Sample data (you should replace this with your own dataset)\n",
    "#num_samples = len(cores_t_plus_xhr[0,0,:])\n",
    "#sequence_length = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d8ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the input layer with no definite frame size.\n",
    "inp = layers.Input(shape=(None, image_height,image_width, num_channels))\n",
    "\n",
    "# We will construct 3 `ConvLSTM2D` layers with batch normalization,\n",
    "# followed by a `Conv3D` layer for the spatiotemporal outputs.\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=8, #64,\n",
    "    kernel_size=(5, 5),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(inp)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=8,#64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=8,#64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding=\"same\",\n",
    "    return_sequences=True,\n",
    "    activation=\"relu\",\n",
    ")(x)\n",
    "x = layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation=\"sigmoid\", padding=\"same\"\n",
    ")(x)\n",
    "\n",
    "# Next, we will build the complete model and compile it.\n",
    "model = keras.models.Model(inp, x)\n",
    "model.compile(\n",
    "    loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "936299ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 128, 256,   0         \n",
      "                             1)]                                 \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, None, 128, 256, 8  7232      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, None, 128, 256, 8  32       \n",
      " ormalization)               )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, None, 128, 256, 8  4640      \n",
      "                             )                                   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, None, 128, 256, 8  32       \n",
      " hNormalization)             )                                   \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, None, 128, 256, 8  544       \n",
      "                             )                                   \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, None, 128, 256, 1  217       \n",
      "                             )                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,697\n",
      "Trainable params: 12,665\n",
      "Non-trainable params: 32\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f75133",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames= 24  # hours\n",
    "batch_size= int(len(cores_t_0[:,0,0])/num_frames)  # days 720 - 96 one day\n",
    "x_train= np.zeros((batch_size,num_frames,image_height,image_width, num_channels))\n",
    "y_train= np.zeros((batch_size,num_frames,image_height,image_width, num_channels))\n",
    "x_val= np.zeros((batch_size,num_frames,image_height,image_width, num_channels))\n",
    "y_val= np.zeros((batch_size,num_frames,image_height,image_width, num_channels))\n",
    "                   \n",
    "    \n",
    "for i in range(0,batch_size,1):  \n",
    "    x_train[i,:,:,:,0]= cores_t_0[i*num_frames:(i+1)*num_frames,:,:]\n",
    "    y_train[i,:,:,:,0]= cores_t_plus_xhr[i*num_frames:(i+1)*num_frames,:,:]\n",
    "\n",
    "for i in range(0,batch_size,1):  \n",
    "    x_val[i,:,:,:,0]= cores_t_0[i*num_frames:(i+1)*num_frames,:,:]\n",
    "    y_val[i,:,:,:,0] = cores_t_plus_xhr[i*num_frames:(i+1)*num_frames,:,:]\n",
    "\n",
    "           \n",
    "# normalize input data        \n",
    "x_train = x_train/np.max(x_train)\n",
    "x_val = x_val/np.max(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c0f8360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(487, 24, 128, 256, 1)\n",
      "487\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "#print(np.unique(y_train))\n",
    "#print(np.unique(y_val))\n",
    "#batch_size= len(cores_t_0[:,0,0])/num_frames  # days 720 - 96 one day\n",
    "#print((batch_size+1)*num_frames)\n",
    "#print(i)\n",
    "print(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1b6b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 1054s 106s/step - loss: 0.6864 - val_loss: 0.6816 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 1045s 106s/step - loss: 0.6134 - val_loss: 0.6504 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 1045s 106s/step - loss: 0.4041 - val_loss: 0.5598 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 1043s 106s/step - loss: 0.1793 - val_loss: 0.4438 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0640 - val_loss: 0.3802 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0506 - val_loss: 0.3534 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 1043s 106s/step - loss: 0.0467 - val_loss: 0.3363 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0449 - val_loss: 0.3201 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0439 - val_loss: 0.3049 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0444 - val_loss: 0.2845 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0445 - val_loss: 0.2644 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 1043s 106s/step - loss: 0.0428 - val_loss: 0.2614 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 1044s 106s/step - loss: 0.0416 - val_loss: 0.2536 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 1045s 106s/step - loss: 0.0415 - val_loss: 0.2423 - lr: 0.0010\n",
      "Epoch 15/20\n",
      " 6/10 [=================>............] - ETA: 6:10 - loss: 0.0372"
     ]
    }
   ],
   "source": [
    "# Define some callbacks to improve training.\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Define modifiable training hyperparameters.\n",
    "epochs = 20\n",
    "batch_size = 50# batch_size#24\n",
    "\n",
    "# Fit the model to the training data.\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example = x_val[np.random.choice(range(len(x_val)), size=1)[0]]\n",
    "#print(example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafcbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a random example from the validation dataset.\n",
    "# example = val_dataset[np.random.choice(range(len(val_dataset)), size=1)[0]]\n",
    "\n",
    "# using x_val for now \n",
    "frames = x_val[0,:,:,:,:] # \n",
    "original_frames = y_val[0,:,:,:,:] # \n",
    "\n",
    "# Predict a new set of 10 frames.\n",
    "for _ in range(2):\n",
    "    # Extract the model's prediction and post-process it.\n",
    "    new_prediction = model.predict(np.expand_dims(frames, axis=0))\n",
    "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
    "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
    "\n",
    "    # Extend the set of prediction frames.\n",
    "    frames = np.concatenate((frames, predicted_frame), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a5dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a figure for the original and new frames.\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 4))\n",
    "\n",
    "# Plot the original frames.\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(original_frames[idx+24:]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Actual core at t{idx+1}\")\n",
    "#    ax.set_title(f\"Frame {idx + 24}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Plot the new frames.\n",
    "new_frames = frames[24:, ...]\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(new_frames[idx]), cmap=\"gray\")\n",
    "    ax.set_title(f\"Predicted core at t{idx+1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Display the figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a91e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(new_frames[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfeac07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac479cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_frames= 24  # hours\n",
    "#batch_size= len(cores_t_0[:,0,0])/num_frames  # days 720 - 96 one day\n",
    "#num_of_batches = round(len(cores_t_0[:,0,0])/batch_size)\n",
    "#x_train= np.zeros((num_of_batches,num_frames,image_height,image_width, num_channels))\n",
    "#y_train= np.zeros((num_of_batches,num_frames,image_height,image_width, num_channels))\n",
    "#x_val= np.zeros((num_of_batches,num_frames,image_height,image_width, num_channels))\n",
    "#y_val= np.zeros((num_of_batches,num_frames,image_height,image_width, num_channels))\n",
    "                 \n",
    "#for i in range(0,num_of_batches,1): #i=batch number\n",
    "    #x_batch= cores_t_0[i:i+batch_size,:,:]\n",
    "    #y_batch= cores_t_plus_xhr[i:i+batch_size,:,:]\n",
    "    #for j in range(0, num_of_batches, batch_size,num_frames):\n",
    "     #   f=1    \n",
    " #   x_train[i,:,:,:,0]= cores_t_0[i*num_frames:(i+1)*num_frames,:,:]\n",
    "  #  y_train[i,:,:,:,0] = cores_t_plus_xhr[i*num_frames:(i+1)*num_frames,:,:]\n",
    "        \n",
    "#x_train = x_train/np.max(x_train)\n",
    "#        \n",
    "#x_val = \n",
    "#y_val[i+500] = create_shifted_frames(val_dataset)\n",
    "#for i in range(0,num_of_batches,1): #i=batch number\n",
    "    #x_batch= cores_t_0[i:i+batch_size,:,:]\n",
    "    #y_batch= cores_t_plus_xhr[i:i+batch_size,:,:]\n",
    "    #for j in range(0, num_of_batches, batch_size,num_frames):\n",
    "     #   f=1    \n",
    " #   x_val[i,:,:,:,0]= cores_t_0[i*num_frames:(i+1)*num_frames,:,:]\n",
    "  #  y_val[i,:,:,:,0] = cores_t_plus_xhr[i*num_frames:(i+1)*num_frames,:,:]\n",
    "    \n",
    "#x = data[:, 0 : data.shape[1] - 1, :, :]\n",
    "#y = data[:, 1 : data.shape[1], :, :]\n",
    "#batch_size (# of samples for training), num_frames (frames per video), width, height, channels) - \n",
    "#Training Dataset Shapes: (900, 19, 64, 64, 1), (900, 19, 64, 64, 1)\n",
    "#Validation Dataset Shapes: (100, 19, 64, 64, 1), (100, 19, 64, 64, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
